{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Recommender Systems\n",
    "[Yukun Liu](mailto:yukun.liu@telecom-paris.fr), [Xiuhan Su](mailto:xiuhan.su@telecom-paris.fr)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 1. Presentation of the model\n",
    "> #### Question 1.1 \n",
    ">Download the file and check size of R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movielens_utils import *\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.sparse.linalg import svds\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of R is (943, 1682)\n",
      "The mini size of R is (100, 200)\n"
     ]
    }
   ],
   "source": [
    "# filename = \"D:/Telecom 1e anne/Telecom Period 2/SD-TSIA211/TP/ml-100k/u.data\"\n",
    "filename = os.getcwd() + '\\\\ml-100k\\\\u.data'\n",
    "[R, mask] = load_movielens(filename, minidata=False)\n",
    "[mini_R, mini_mask] = load_movielens(filename, minidata=True)\n",
    "\n",
    "print('The size of R is {}'.format(R.shape))\n",
    "print('The mini size of R is {}'.format(mini_R.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R has an original size $(943,1682)$, and it reduces to $(100,200)$ when option **minidata** is on. **minidata** is used to reduce the matrix size R, which can process the iteration speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Question 1.2\n",
    ">How many user and flms are there in the database ? What is the total number of grades ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 943 users and 1682 films in the database, and the total number of grades is 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 2. Find $P$ when $Q^0$ is fixed\n",
    "> #### Question 2.1\n",
    ">Calculate the gradient of function $g$. We will admit that this gradient is Lipschitz continuous with constant $L_0 = \\rho+||Q^0)^TQ^0||_F$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the context, we know\n",
    "$$g(P)=\\frac{1}{2}\\mathop{\\Sigma}_{u\\in U, i \\in I}(R_{u,i}-(\\mathbb{1}_K)_{u,i}\\mathop{\\Sigma}_{f\\in F}Q_{u,f}P_{f,i})^2+\\frac{\\rho}{2}\\mathop{\\Sigma}_{u\\in U, f \\in F}Q_{u,f}^2+\\frac{\\rho}{2}\\mathop{\\Sigma}_{i\\in I, f \\in F}P_{f,i}^2$$\n",
    "\n",
    "Furthermore its gradient $\\triangledown g=\\frac{\\partial g(P)}{\\partial P}$ is a $F$ by $I$ matrix. While the (m,n) element of gradient $\\triangledown g$ can be denoted by\n",
    "\n",
    "$$\\begin{aligned}\n",
    "[\\triangledown g][\\triangledown g]_{m,n}&=\\frac{\\partial g(P)}{\\partial P_{m,n}}\\\\\n",
    "&=\\mathop{\\Sigma}_{u\\in U}Q^0_{u,m}(\\mathbb{1}_K)_{u,n}((\\mathbb{1}_K)_{u,n}\\mathop{\\Sigma}_{f\\in F}Q^0_{u,f}P_{f,n}-R_{u,n})+\\rho P_{m,n}\\\\\n",
    "% &=\\mathop{\\Sigma}_{u\\in U}(Q^0)^T_{m,u}(\\mathbb{1}_K)_{u,n}(\\mathop{\\Sigma}_{f\\in F}Q^0_{u,f}P_{f,n}-R_{u,n})+\\rho P_{m,n}\\\\\n",
    "&=\\mathop{\\Sigma}_{u\\in U}(Q^0)^T_{m,u}(\\mathbb{1}_K)_{u,n}(Q^0P)_{u,n}-(Q^0)^T_{m,:}R_{:,n}+\\rho P_{m,n}\\\\\n",
    "&=\\mathop{\\Sigma}_{u\\in U}(Q^0)^T_{m,u}(\\mathbb{1}_K\\circ Q^0P)_{u,n}-(Q^0)^T_{m,:}R_{:,n}+\\rho P_{m,n}\\\\\n",
    "% &=\\mathop{\\Sigma}_{f\\in F}\\mathop{\\Sigma}_{u\\in U}(Q^0)^T_{m,u}Q^0_{u,f}(\\mathbb{1}_K)_{u,n}P_{f,n}-(Q^0_{m,:})^TR_{:,n}+\\rho P_{m,n}\\\\\n",
    "% &=\\mathop{\\Sigma}_{f\\in F}[(Q^0)^TQ^0]_{m,f}P_{f,n}(\\mathbb{1}_K)_{u,n}-(Q^0_{m,:})^TR_{:,n}+\\rho P_{m,n}\\\\\n",
    "% &=[(Q^0)^T]_{m,:}(\\mathbb{1}_K\\circ Q^0P)_{:,n}-(Q^0)^T_{m,:}R_{:,n}+\\rho P_{m,n}\\\\\n",
    "&= [(Q^0)^T(\\mathbb{1}_K\\circ Q^0P)]_{m,n}-[(Q^0)^TR]_{m,n}+\\rho P_{m,n}\n",
    "\\end{aligned}$$\n",
    "\n",
    "And the gradient can be formulated as\n",
    "$$\\begin{aligned}\n",
    "\\triangledown g(P) &= (Q^0)^T(\\mathbb{1}_K\\circ Q^0P)-(Q^0)^TR+\\rho P\\\\\n",
    "&= (Q^0)^T(\\mathbb{1}_K\\circ Q^0P-R)+\\rho P\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Question 2.2\n",
    ">The function objective provided in the file _movielens utils.py_ computes g(P). Complete\n",
    "this function so that it also computes $\\triangledown g(P)$. You may check your calculations with the\n",
    "function _scipy.optimize.check grad_ (you may need _numpy.reshape_ and _numpy.ravel_\n",
    "because check grad does not accept matrix variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(P):\n",
    "    '''\n",
    "    cost function w.r.t Q\n",
    "    P : column vector of size(1, F*C)\n",
    "    mask, R taken as given\n",
    "    '''\n",
    "    if P.size == 6728:\n",
    "        P = P.reshape(4, 1682)\n",
    "        \n",
    "    tmp = (R - Q0.dot(P)) * mask\n",
    "    value = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q0 ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    return value\n",
    "\n",
    "def grad(P, ravel=True):\n",
    "    '''\n",
    "    gradient function w.r.t Q\n",
    "    P : column vector of size(1, F*C)\n",
    "    ravel : True when P is ravelled to a column vector\n",
    "    '''\n",
    "    \n",
    "    if ravel:\n",
    "        P = P.reshape(4, 1682)\n",
    "        tmp = (R - Q0.dot(P)) * mask \n",
    "        grad_P = np.ravel(Q0.T.dot(-tmp) + rho * P)\n",
    "\n",
    "    else:\n",
    "        tmp = (R - Q0.dot(P)) * mask \n",
    "        grad_P = Q0.T.dot(-tmp) + rho * P            \n",
    "\n",
    "    return  grad_P   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f146bdf93fe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_P\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The gradient error is {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\lenovo\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mcheck_grad\u001b[1;34m(func, grad, x0, *args, **kwargs)\u001b[0m\n\u001b[0;32m    902\u001b[0m                          (list(kwargs.keys()),))\n\u001b[0;32m    903\u001b[0m     return sqrt(sum((grad(x0, *args) -\n\u001b[1;32m--> 904\u001b[1;33m                      approx_fprime(x0, func, step, *args))**2))\n\u001b[0m\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\lenovo\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mapprox_fprime\u001b[1;34m(xk, f, epsilon, *args)\u001b[0m\n\u001b[0;32m    852\u001b[0m                              \"return a scalar value.\")\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m     return approx_derivative(f, xk, method='2-point', abs_step=epsilon,\n\u001b[0m\u001b[0;32m    855\u001b[0m                              args=args, f0=f0)\n\u001b[0;32m    856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\lenovo\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msparsity\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m             return _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0m\u001b[0;32m    427\u001b[0m                                      use_one_sided, method)\n\u001b[0;32m    428\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\lenovo\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\u001b[0m in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Recompute dx as exactly representable number.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'3-point'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0muse_one_sided\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\lenovo\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             raise RuntimeError(\"`fun` return value has \"\n",
      "\u001b[1;32m<ipython-input-3-5743d3b66f84>\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(P)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mR\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mQ0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrho\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ0\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rho = 0.5\n",
    "\n",
    "Q0,s,P0 = svds(R, k=4)\n",
    "\n",
    "[val, grad_P] = objective(P0, Q0, R, mask, rho)\n",
    "\n",
    "err = check_grad(func, grad, np.ravel(P0))\n",
    "\n",
    "print('The gradient error is {}'.format(err))\n",
    "print('The norm is {}'.format( np.linalg.norm(grad_P) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the norm of gradient, the error is relatively small. Then a conclusion can be drawn that our function of gradient is derived in the right way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Question 2.3 \n",
    ">Code a function `gradient(g, P0, gamma, epsilon)` that minimizes a function g using the gradient method with a constant step size, $\\gamma$, starting from the initial point $P^0$ and with stopping criterion $||\\triangledown g(P_k)||_F\\le \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Question 2.4\n",
    "Run the function coded in the previous question in order to minimize g up to the precision $\\epsilon = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the gradient is Lipschitz continuous with constant $L_0 = \\rho+||Q^0)^TQ^0||_F$, the constant step size will be set as $$\\gamma = \\frac{2}{L_0}$$\n",
    "\n",
    "The sequence of points $(P_k)_{k \\in \\mathbb{N}}$ in $\\mathbb{R}^{F\\times I}$ will be defined by\n",
    "$$P_{k+1}=P_k-\\gamma \\triangledown g(P_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(P, gamma, epsilon, a = None, bcoef = None):\n",
    "    \"\"\"\n",
    "    minimizes a function g using the gradient method\n",
    "    P : initial point\n",
    "    gamma : constant step size\n",
    "    epsilon : exit criterion\n",
    "    lsch : True when applying Taylor based line search\n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0 # step size\n",
    "    g = grad(P, ravel=False)   \n",
    "    \n",
    "    while np.linalg.norm(g) > epsilon :\n",
    "        counter += 1            \n",
    "\n",
    "        if a != None and bcoef != None:\n",
    "            \"\"\"\n",
    "            line search method calculating step size\n",
    "            \"\"\"\n",
    "            gamma = find_step(g, P, gamma, a, bcoef)  \n",
    "              \n",
    "        P = P - gamma * g            \n",
    "        g = grad(P, ravel=False)\n",
    "\n",
    "    return g, counter\n",
    "\n",
    "def find_step(g, P, gamma0, a, bcoef):\n",
    "    b = bcoef * gamma0\n",
    "    for l in range(0,100):\n",
    "        \n",
    "        gamma = b * a**l\n",
    "        measure = func(P) - func(P - gamma*g) - 1/2 * gamma * np.linalg.norm(g)**2\n",
    "\n",
    "        if measure > 0:\n",
    "            break\n",
    "\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "L0 = rho + np.linalg.norm( Q0.T.dot(Q0) )\n",
    "Q0,s,P0 = svds(R, k=4)\n",
    "Gamma  = 2 / L0\n",
    "G0 = grad(P0, ravel=False)\n",
    "Epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 0.6258549235496634\n",
      "The optimality is achieved in 8 steps\n"
     ]
    }
   ],
   "source": [
    "# constant step \n",
    "gout = gradient(P0, Gamma, Epsilon)\n",
    "\n",
    "print('The precision is {}'.format(np.linalg.norm(gout[0])))\n",
    "print('The optimality is achieved in {} steps'.format(gout[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Question 2.5\n",
    "Add a line search to your gradient method, so that you do not rely on the Lipschitz constant of the gradient any more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the Talyor-based line search, we have\n",
    "\n",
    "$$g(P^+(\\gamma_k))\\le g(P_k)+\\langle\\triangledown g(P_k), P^+(\\gamma_k)-P_k\\rangle+\\frac{1}{2\\gamma_k}\\|P_k-P^+(\\gamma_k)\\|^2,k\\in\\{1,\\cdots,N\\}$$\n",
    "\n",
    "where $P^+(\\gamma_k)=P_k-\\gamma_k\\triangledown g(P_k)$ and $\\gamma_k = ba^l,b>0, a\\in(0,1)$, $l$ is bounded by the criterion that $L'=\\frac{1}{\\gamma_k}\\ge L$.\n",
    "\n",
    "Since $P^+(\\gamma_k)-P_k=-\\gamma_k\\triangledown g(P_k)$, the inequality can be formulated as \n",
    "\n",
    "$$ g(P_k)-g(P^+(\\gamma_k))-\\frac{\\gamma_k}{2}\\|\\triangledown g(P_k)\\|^2\\ge 0$$ $l$ is the smallest integer makes the inequality true. In the implementation, we chose the classical parameter: $a=0.5,b=2\\gamma_k$\n",
    "\n",
    "<!-- Let's set $$\\gamma_k = ba^l,k\\in\\{1,\\cdots,N\\}$$ where $a=0.5, b=2\\gamma_{k-1},l=\\lfloor \\frac{\\log(ba^{-1}L_0)}{\\log(a^{-1})}\\rfloor, \\gamma_0=\\frac{2}{L_0}$ to ensure the chosing step won't break the Lipschitz continuity.\n",
    "\n",
    "Since $L_0 = \\rho+||Q^0)^TQ^0||_F=2.5, \\gamma_0=\\frac{2}{L_0}=0.8,$ $l$ is chosen to be $1$ in this case. Besides, for those cases $L_0$ is unknown, $l=1$ will always be the least choice which indicates constant step size.\n",
    "Then \n",
    "$$P^+(\\gamma_k)=P_k-\\gamma_k\\triangledown g(P_k)$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is 0.25486949872124526\n",
      "The optimality is achieved in 7 steps\n"
     ]
    }
   ],
   "source": [
    "# linsearch\n",
    "gout_lsch = gradient(P0, Gamma, Epsilon, a=0.5, bcoef=2)\n",
    "\n",
    "print('The precision is {}'.format(np.linalg.norm(gout_lsch[0])))\n",
    "print('The optimality is achieved in {} steps'.format(gout_lsch[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Resolution of the full problem\n",
    "> #### Question 3.1 \n",
    "Let f be the function defined by $f(P,Q) = \\frac{1}{2}||\\mathbb{1}_K \\circ(R-QP)||^2_F+\\frac{\\rho}{2}||Q||^2_F+\\frac{\\rho}{2}||P||^2_F$. \n",
    ">\n",
    ">By remarking that f is a polynomial of degree 4, show that its gradient is not Lipschitz\n",
    "continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $f$ is expressed as\n",
    "$$f(P,Q)=\\frac{1}{2}\\mathop{\\Sigma}_{u\\in U, i \\in I}(R^2_{u,i}-2(\\mathbb{1}_K)_{u,i}R_{u,i}\\mathop{\\Sigma}_{f\\in F}Q_{u,f}P_{f,i}+\\mathop{\\Sigma}_{a,b\\in F}Q_{u,a}P_{a,i}Q_{u,b}P_{b,i})+\\frac{\\rho}{2}\\mathop{\\Sigma}_{u\\in U, f \\in F}Q_{u,f}^2+\\frac{\\rho}{2}\\mathop{\\Sigma}_{i\\in I, f \\in F}P_{f,i}^2$$\n",
    "\n",
    "And we can tell the mysterious term $\\mathop{\\Sigma}_{a,b\\in F}Q_{u,a}P_{a,i}Q_{u,b}P_{b,i}$ is with order of 4. As a result, $f$ is not Lipschitz continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Question 3.2\n",
    "Solve Problem (1) by the gradient method with line search until reaching the precision\n",
    "$\\|\\triangledown f(P_k,Q_k)\\|_F \\le\\epsilon$ with $\\epsilon = 100$. How do you interpret what the algorithm returns ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of function $f$ w.r.t $Q$ can be derived in a similar way as Prob.(2.1), then we have\n",
    "\n",
    "$$\\frac{\\partial f(P,Q)}{\\partial P}= Q^T(\\mathbb{1}_K\\circ QP-R)+\\rho P$$\n",
    "$$\\frac{\\partial f(P,Q)}{\\partial Q}= (\\mathbb{1}_K\\circ QP-R)P^T+\\rho Q$$\n",
    "while $\\frac{\\partial f(P,Q)}{\\partial P}\\in\\mathbb{R}^{F\\times I},\\frac{\\partial f(P,Q)}{\\partial Q}\\in\\mathbb{R}^{U\\times F}$.\n",
    "And the overall gradient is \n",
    "$$\\triangledown f(P,Q)=\\\n",
    "\\begin{bmatrix}\\frac{\\partial f(P,Q)}{\\partial P}\\\\ \\frac{\\partial f^T(P,Q)}{\\partial Q}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Similary, to implement the Taylor based line search, we'll find the step $\\gamma_k=ba^l$ s.t.\n",
    "$$ f(P_k,Q_k)-f(P^+(\\gamma_k),Q^+(\\gamma_k))-\\frac{\\gamma_k}{2}\\|\\triangledown f(P_k,Q_k)\\|^2\\ge 0$$ $l$ is the smallest integer makes the inequality true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val0, grad_P0, grad_Q0 = total_objective(P0, Q0, R, mask, rho)\n",
    "\n",
    "PQvec_ini = np.concatenate([grad_P0.ravel(), grad_Q0.ravel()])\n",
    "val0, PQvec_1 = total_objective_vectorized(PQvec_ini, R, mask, rho)\n",
    "val1, PQvec_2 = total_objective_vectorized(PQvec_1, R, mask, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1843600334971337e+32\n",
      "-1.3652047008696042e+31\n",
      "-8.532275383971703e+29\n",
      "-5.33235463031049e+28\n",
      "-3.332324815372113e+27\n",
      "-2.0822070420597326e+26\n",
      "-1.30075961225775e+25\n",
      "-8.122004473246104e+23\n",
      "-5.0665845622077305e+22\n",
      "-3.1545566578665094e+21\n",
      "-1.9565909622169613e+20\n",
      "-1.204276319132169e+19\n",
      "-7.298435893671085e+17\n",
      "-4.286365332688026e+16\n",
      "-2360060243133986.0\n",
      "-113651749583715.98\n",
      "-4252755822412.793\n",
      "-193661790249.08493\n",
      "-43589778067.41798\n",
      "-1515440198.872284\n",
      "8977751350.48817\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.5367431640625e-07"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 100\n",
    "PQvec_gradini = np.concatenate([grad_P0.ravel(), grad_Q0.ravel()])\n",
    "gamma_test = 0.5\n",
    "find_gamma(PQvec_gradini, gamma_test, 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gamma(PQvec, gamma0, a, bcoef):\n",
    "    \n",
    "    b = bcoef * gamma0\n",
    "    \n",
    "    for l in range(0,100):\n",
    "        \n",
    "        gamma = b * a**l\n",
    "        val0, PQvec_grad = total_objective_vectorized(PQvec, R, mask, rho)\n",
    "        PQvec_new = PQvec - gamma * PQvec_grad\n",
    "        \n",
    "        val1, PQvec_grad1 = total_objective_vectorized(PQvec_new, R, mask, rho)\n",
    "        \n",
    "        measure = val0 - val1 - 1/2 * gamma * np.linalg.norm(PQvec_grad)**2\n",
    "\n",
    "        if measure > 0:\n",
    "#             print(l)\n",
    "            break\n",
    "\n",
    "    return gamma, PQvec_new, PQvec_grad\n",
    "\n",
    "def gradient_all(P, Q = None, gamma0, epsilon, a = None, bcoef = None):\n",
    "    \"\"\"\n",
    "    minimizes a function g using the gradient method\n",
    "    P : initial point\n",
    "    gamma : constant step size\n",
    "    epsilon : exit criterion\n",
    "    lsch : True when applying Taylor based line search\n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0 # step size\n",
    "    \n",
    "    PQvec = np.concatenate([P.ravel(), Q.ravel()])\n",
    "    \n",
    "    val0, grad_P0, grad_Q0 = total_objective(P, Q, R, mask, rho)\n",
    "    \n",
    "    PQvec_grad = np.concatenate([grad_P0.ravel(), grad_Q0.ravel()])\n",
    "    \n",
    "    while np.linalg.norm(PQvec_grad) > epsilon :\n",
    "        counter += 1            \n",
    "\n",
    "        if a != None and bcoef != None:\n",
    "            \"\"\"\n",
    "            line search method calculating step size\n",
    "            \"\"\"e\n",
    "            gamma0, PQvec, PQvec_grad = find_gamma(PQvec, gamma0, a, bcoef)  \n",
    "              \n",
    "    return np.linalg.norm(PQvec_grad), counter, PQvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 100\n",
    "err, steps, PQvec = gradient_all(P0, Q0, gamma_test, epsilon, a = 0.5, bcoef = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = R.shape[1]\n",
    "n_users = R.shape[0]\n",
    "F = PQvec.shape[0] // (n_items + n_users)\n",
    "\n",
    "Pvec = PQvec[0:n_items*F]\n",
    "Qvec = PQvec[n_items*F:]\n",
    "\n",
    "P = np.reshape(Pvec, (F, n_items))\n",
    "Q = np.reshape(Qvec, (n_users, F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = Q.dot(P)\n",
    "\n",
    "df = pd.DataFrame(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "index =  df[300][df.iloc[300] > 4].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned $Q$ and $P$ indicate the matrixes make the term $QP$ **most close** to our observation $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Question 3.3\n",
    "What film would you recommend to user 300 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 97.13320622890856)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "user = 299\n",
    "thres = 4.5 # the rating no less than thres will be recommended\n",
    "\n",
    "rec_index =  df[user][df.iloc[user] > thres].index.tolist()\n",
    "print(len(rec_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For User 300, we will recommend the film in index\n",
    "$[99, 256, 287, 293, 299, 327]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
